import os
import random 
import gradio as gr
import numpy as np
import time
import torch, torchaudio
import gc
import warnings
warnings.filterwarnings('ignore')
from zhconv import convert
from LLM import LLM
from TTS import EdgeTTS
from src.cost_time import calculate_time
from configs import *
from fastapi import FastAPI
from starlette.middleware.cors import CORSMiddleware

os.environ["GRADIO_TEMP_DIR"]= './temp'
os.environ["WEBUI"] = "true"

def get_title(title = 'Linly æ™ºèƒ½å¯¹è¯ç³»ç»Ÿ (Linly-Talker)'):
    description = f"""
    <p style="text-align: center; font-weight: bold;">
        <span style="font-size: 28px;">{title}</span>
        <br>
        <span style="font-size: 18px;" id="paper-info">
            [<a href="https://zhuanlan.zhihu.com/p/671006998" target="_blank">çŸ¥ä¹</a>]
            [<a href="https://www.bilibili.com/video/BV1rN4y1a76x/" target="_blank">bilibili</a>]
            [<a href="https://github.com/Kedreamix/Linly-Talker" target="_blank">GitHub</a>]
            [<a herf="https://kedreamix.github.io/" target="_blank">ä¸ªäººä¸»é¡µ</a>]
        </span>
        <br> 
        <span>Linly-Talkeræ˜¯ä¸€æ¬¾åˆ›æ–°çš„æ•°å­—äººå¯¹è¯ç³»ç»Ÿï¼Œå®ƒèåˆäº†æœ€æ–°çš„äººå·¥æ™ºèƒ½æŠ€æœ¯ï¼ŒåŒ…æ‹¬å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰ğŸ¤–ã€è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰ğŸ™ï¸ã€æ–‡æœ¬åˆ°è¯­éŸ³è½¬æ¢ï¼ˆTTSï¼‰ğŸ—£ï¸å’Œè¯­éŸ³å…‹éš†æŠ€æœ¯ğŸ¤ã€‚</span>
    </p>
    """
    return description

# Default system and prompt settings
DEFAULT_SYSTEM = 'ä½ æ˜¯ä¸€ä¸ªå¾ˆæœ‰å¸®åŠ©çš„åŠ©æ‰‹'
PREFIX_PROMPT = 'è¯·ç”¨å°‘äº25ä¸ªå­—å›ç­”ä»¥ä¸‹é—®é¢˜\n\n'
# Default parameters
IMAGE_SIZE = 256
PREPROCESS_TYPE = 'crop'
FACERENDER = 'facevid2vid'
ENHANCER = False
IS_STILL_MODE = False
EXP_WEIGHT = 1
USE_REF_VIDEO = False
REF_VIDEO = None
REF_INFO = 'pose'
USE_IDLE_MODE = False
AUDIO_LENGTH = 5

edgetts = EdgeTTS()

@calculate_time
def Asr(audio):
    try:
        question = asr.transcribe(audio)
        question = convert(question, 'zh-cn')
    except Exception as e:
        gr.Warning("ASR Error: ", e)
        question = 'Gradioå­˜åœ¨ä¸€äº›bugï¼Œéº¦å…‹é£æ¨¡å¼æœ‰æ—¶å€™å¯èƒ½éŸ³é¢‘è¿˜æœªä¼ å…¥ï¼Œè¯·é‡æ–°ç‚¹å‡»ä¸€ä¸‹è¯­éŸ³è¯†åˆ«å³å¯'
    return question

def clear_memory():
    """
    æ¸…ç†PyTorchçš„æ˜¾å­˜å’Œç³»ç»Ÿå†…å­˜ç¼“å­˜ã€‚
    """
    # 1. æ¸…ç†ç¼“å­˜çš„å˜é‡
    gc.collect()  # è§¦å‘Pythonåƒåœ¾å›æ”¶
    torch.cuda.empty_cache()  # æ¸…ç†PyTorchçš„æ˜¾å­˜ç¼“å­˜
    torch.cuda.ipc_collect()  # æ¸…ç†PyTorchçš„è·¨è¿›ç¨‹é€šä¿¡ç¼“å­˜
    
    # 2. æ‰“å°æ˜¾å­˜ä½¿ç”¨æƒ…å†µï¼ˆå¯é€‰ï¼‰
    print(f"Memory allocated: {torch.cuda.memory_allocated() / (1024 ** 2):.2f} MB")
    print(f"Max memory allocated: {torch.cuda.max_memory_allocated() / (1024 ** 2):.2f} MB")
    print(f"Cached memory: {torch.cuda.memory_reserved() / (1024 ** 2):.2f} MB")
    print(f"Max cached memory: {torch.cuda.max_memory_reserved() / (1024 ** 2):.2f} MB")

def generate_seed():
    seed = random.randint(1, 100000000)
    return {"__type__": "update", "value": seed}

def set_all_random_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

def change_instruction(mode):
    return instruct_dict.get(mode, 'æœªçŸ¥æ¨¡å¼')

PROMPT_SR, TARGET_SR = 16000, 22050
DEFAULT_DATA = np.zeros(TARGET_SR)

@calculate_time
def TTS_response(text, voice, rate, volume, pitch,
                tts_method='Edge-TTS', save_path='answer.wav'):
    if text == '':
        text = 'è¯·è¾“å…¥æ–‡å­—/é—®é¢˜'
    if tts_method == 'Edge-TTS':
        if not edgetts.network:
            gr.Warning("è¯·æ£€æŸ¥ç½‘ç»œæˆ–ä½¿ç”¨å…¶ä»–æ¨¡å‹ï¼Œä¾‹å¦‚PaddleTTS")
            return None
        try:
            edgetts.predict(text, voice, rate, volume, pitch, save_path, 'answer.vtt')
        except Exception as e:
            os.system(f'edge-tts --text "{text}" --voice {voice} --write-media {save_path} --write-subtitles answer.vtt')
        return save_path
    else:
        gr.Warning('æœªçŸ¥æ¨¡å‹')
    return None

inference_mode_list = ['é¢„è®­ç»ƒéŸ³è‰²', '3sæé€Ÿå¤åˆ»', 'è·¨è¯­ç§å¤åˆ»']
instruct_dict = {'é¢„è®­ç»ƒéŸ³è‰²': '1. é€‰æ‹©é¢„è®­ç»ƒéŸ³è‰²\n2. ç‚¹å‡»ç”ŸæˆéŸ³é¢‘æŒ‰é’®',
                '3sæé€Ÿå¤åˆ»': '1. é€‰æ‹©promptéŸ³é¢‘æ–‡ä»¶ï¼Œæˆ–å½•å…¥promptéŸ³é¢‘ï¼Œæ³¨æ„ä¸è¶…è¿‡30sï¼Œè‹¥åŒæ—¶æä¾›ï¼Œä¼˜å…ˆé€‰æ‹©promptéŸ³é¢‘æ–‡ä»¶\n2. è¾“å…¥promptæ–‡æœ¬\n3. ç‚¹å‡»ç”ŸæˆéŸ³é¢‘æŒ‰é’®',
                'è·¨è¯­ç§å¤åˆ»': '1. é€‰æ‹©promptéŸ³é¢‘æ–‡ä»¶ï¼Œæˆ–å½•å…¥promptéŸ³é¢‘ï¼Œæ³¨æ„ä¸è¶…è¿‡30sï¼Œè‹¥åŒæ—¶æä¾›ï¼Œä¼˜å…ˆé€‰æ‹©promptéŸ³é¢‘æ–‡ä»¶\n2. ç‚¹å‡»ç”ŸæˆéŸ³é¢‘æŒ‰é’®',
                'è‡ªç„¶è¯­è¨€æ§åˆ¶': '1. é€‰æ‹©é¢„è®­ç»ƒéŸ³è‰²\n2. è¾“å…¥instructæ–‡æœ¬\n3. ç‚¹å‡»ç”ŸæˆéŸ³é¢‘æŒ‰é’®'}


@calculate_time
def LLM_response(
    question,  # è¾“å…¥çš„éŸ³é¢‘å’Œæ–‡æœ¬é—®é¢˜
    voice,  # è¯­éŸ³åˆæˆå‚æ•°
    tts_method='Edge-TTS'  # TTS æ–¹æ³•ï¼Œé»˜è®¤ä½¿ç”¨ 'Edge-TTS'
):
    if len(question) == 0:
        gr.Warning("è¯·è¾“å…¥é—®é¢˜")
        return None, None, None

    # ç”Ÿæˆå›ç­”
    answer = llm.generate(question, DEFAULT_SYSTEM)
    print("LLM å›å¤ï¼š", answer)

    # åˆæˆå›ç­”è¯­éŸ³
    tts_audio = TTS_response(
        answer, voice, 0, 100, 0
    )

    # ç”ŸæˆVTTæ–‡ä»¶ï¼ˆå¦‚æœTTSæ–¹æ³•ä¸º'Edge-TTS'ï¼‰
    tts_vtt = 'answer.vtt' if tts_method == 'Edge-TTS' else None

    return tts_audio, tts_vtt, answer

@calculate_time
def Talker_response_img(text):


    driven_audio, driven_vtt, _ = LLM_response(text, 'zh-CN-XiaoxiaoNeural',
                                               'Edge-TTS')

    if driven_audio is None:
        gr.Warning("éŸ³é¢‘æ²¡æœ‰æ­£å¸¸ç”Ÿæˆï¼Œè¯·æ£€æŸ¥TTSæ˜¯å¦æ­£ç¡®")
        return None

    # è§†é¢‘ç”Ÿæˆ
    #video = None
    video = talker.test2('/root/autodl-tmp/Linly-Talker/myface.png', driven_audio, 'crop', False, False,
                             2, 256, 0, True, 1,
                             REF_VIDEO, REF_INFO, USE_IDLE_MODE, AUDIO_LENGTH, True, 
                             fps=20)
    """ if method == 'SadTalker':
        video = talker.test2(source_image, driven_audio, 'crop', False, False,
                             2, 256, 0, True, 1,
                             REF_VIDEO, REF_INFO, USE_IDLE_MODE, AUDIO_LENGTH, True, 
                             fps=20) """
    """ else:
        gr.Warning("ä¸æ”¯æŒçš„æ–¹æ³•ï¼š" + method) """
        #return None

    return (video, driven_vtt) if driven_vtt else video

def chat_response(system, message, history):
    # response = llm.generate(message)
    response, history = llm.chat(system, message, history)
    print(history)
    # æµå¼è¾“å‡º
    for i in range(len(response)):
        time.sleep(0.01)
        yield "", history[:-1] + [(message, response[:i+1])]
    return "", history

def modify_system_session(system: str) -> str:
    if system is None or len(system) == 0:
        system = default_system
    llm.clear_history()
    return system, system, []

def clear_session():
    # clear history
    llm.clear_history()
    return '', []


GPT_SoVITS_ckpt = "GPT_SoVITS/pretrained_models"



iface = gr.Interface(
    fn=Talker_response_img,
    inputs=[gr.Textbox(label="è¾“å…¥æ–‡å­—/é—®é¢˜", lines=3, placeholder='è¯·è¾“å…¥æ–‡æœ¬æˆ–é—®é¢˜ï¼ŒåŒæ—¶å¯ä»¥è®¾ç½®LLMæ¨¡å‹ã€‚é»˜è®¤ä½¿ç”¨ç›´æ¥å›å¤ã€‚')], 
    outputs=[gr.Video(label="æ•°å­—äººè§†é¢‘", format="mp4")],
    title="Image and Text to Video",  # Title of the app
    description="Upload an image and enter a caption to create a video that displays the image with the caption for 3 seconds."  # Description
)



def success_print(text):
    """è¾“å‡ºç»¿è‰²æ–‡æœ¬ï¼Œè¡¨ç¤ºæˆåŠŸä¿¡æ¯ã€‚"""
    print(f"\033[1;32m{text}\033[0m")

def error_print(text):
    """è¾“å‡ºçº¢è‰²æ–‡æœ¬ï¼Œè¡¨ç¤ºé”™è¯¯ä¿¡æ¯ã€‚"""
    print(f"\033[1;31m{text}\033[0m")
    
if __name__ == "__main__":
    # åˆå§‹åŒ–LLMç±»
    llm_class = LLM(mode='offline')
    llm = llm_class.init_model('ç›´æ¥å›å¤ Direct Reply')
    success_print("é»˜è®¤ä¸ä½¿ç”¨LLMæ¨¡å‹ï¼Œç›´æ¥å›å¤é—®é¢˜ï¼ŒåŒæ—¶å‡å°‘æ˜¾å­˜å ç”¨ï¼")

    # å°è¯•åŠ è½½GPT-SoVITSæ¨¡å—


    # å°è¯•åŠ è½½SadTalkeræ¨¡å—
    try:
        from TFG import SadTalker
        talker = SadTalker(lazy_load=True)
        success_print("Success! SadTalkeræ¨¡å—åŠ è½½æˆåŠŸï¼Œé»˜è®¤ä½¿ç”¨SadTalkeræ¨¡å‹")
    except Exception as e:
        error_print(f"SadTalker åŠ è½½å¤±è´¥: {e}")
        error_print("å¦‚æœä½¿ç”¨SadTalkerï¼Œè¯·å…ˆä¸‹è½½SadTalkeræ¨¡å‹")

    # å°è¯•åŠ è½½Whisper ASRæ¨¡å—
    try:
        from ASR import WhisperASR
        asr = WhisperASR('base')
        success_print("Success! WhisperASRæ¨¡å—åŠ è½½æˆåŠŸï¼Œé»˜è®¤ä½¿ç”¨Whisper-baseæ¨¡å‹")
    except Exception as e:
        error_print(f"WhisperASR åŠ è½½å¤±è´¥: {e}")
        error_print("å¦‚æœä½¿ç”¨FunASRï¼Œè¯·å…ˆä¸‹è½½WhisperASRæ¨¡å‹å¹¶å®‰è£…ç¯å¢ƒ")

    # æ£€æŸ¥GPUæ˜¾å­˜
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024 ** 3)  # Convert bytes to GB
        if gpu_memory < 8:
            error_print("è­¦å‘Š: æ‚¨çš„æ˜¾å¡æ˜¾å­˜å°äº8GBï¼Œä¸å»ºè®®ä½¿ç”¨MuseTalkåŠŸèƒ½")

    # å°è¯•åŠ è½½MuseTalkæ¨¡å—
    
    # å°è¯•åŠ è½½EdgeTTSæ¨¡å—
    try:
        tts = edgetts
        if not tts.network:
            error_print("EdgeTTSæ¨¡å—åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
    except Exception as e:
        error_print(f"EdgeTTS åŠ è½½å¤±è´¥: {e}")

    # Gradio UIçš„åˆå§‹åŒ–å’Œå¯åŠ¨
    gr.close_all()
    """ demo_img = app_img()
    demo = gr.TabbedInterface(
        interface_list=[demo_img],
        tab_names=["ä¸ªæ€§åŒ–è§’è‰²äº’åŠ¨"],
        title="Linly-Talker WebUI"
    ) """
    """ demo.queue(max_size=4, default_concurrency_limit=2) """
    """ demo.launch(
        server_name=ip,  # æœ¬åœ°localhost:127.0.0.1 æˆ– "0.0.0.0" è¿›è¡Œå…¨å±€ç«¯å£è½¬å‘
        server_port=port,
        # ssl_certfile=ssl_certfile,  # SSLè¯ä¹¦æ–‡ä»¶
        # ssl_keyfile=ssl_keyfile,  # SSLå¯†é’¥æ–‡ä»¶
        # ssl_verify=False,
        share=True,
        debug=True,
    ) """
    import uvicorn

    app = FastAPI()
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["https://90bc-83-147-15-235.ngrok-free.app"],
        allow_credentials=False,
        allow_methods=["*"],
        allow_headers=["*"],
    )

    app1 = gr.mount_gradio_app(app, iface, path='/')
    uvicorn.run(app1, port=port, host=ip)
    """ iface.launch(
        server_name=ip,  # æœ¬åœ°localhost:127.0.0.1 æˆ– "0.0.0.0" è¿›è¡Œå…¨å±€ç«¯å£è½¬å‘
        server_port=port,
        # ssl_certfile=ssl_certfile,  # SSLè¯ä¹¦æ–‡ä»¶
        # ssl_keyfile=ssl_keyfile,  # SSLå¯†é’¥æ–‡ä»¶
        # ssl_verify=False,
        share=True,
        debug=True,
    ) """